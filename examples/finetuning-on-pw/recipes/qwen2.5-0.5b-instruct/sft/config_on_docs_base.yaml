# Script config
dataset_name: "data/wiki-v1"
split_list: [
    "depth_20_size_50_seed_10",
    "depth_20_size_50_seed_11",
    "depth_20_size_50_seed_12",
    "depth_20_size_50_seed_13",
    # "depth_20_size_50_seed_14",  # split not present
    "depth_20_size_50_seed_15",
    "depth_20_size_50_seed_16",
    "depth_20_size_50_seed_17",
    "depth_20_size_50_seed_18",
    "depth_20_size_50_seed_19",
    "depth_20_size_50_seed_20",
  ]
from_local: true
eval_dataset_name: "kilian-group/phantom-wiki-v1"
eval_split_list: ["depth_20_size_50_seed_1"]
eval_from_local: false

# Model arguments
model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
bf16: true
torch_dtype: bfloat16

# Trainer arguments
num_train_epochs: 3
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
packing: false
# size_25 universe is about 2500 tokens, so we set the max length to 4096
max_length: 4096

# Evaluation config
bf16_full_eval: true
# eval_on_start: true
eval_strategy: "steps"
eval_steps: 200
per_device_eval_batch_size: 2

# Logging config
log_level: "info"
logging_strategy: "steps"
logging_first_step: true
logging_steps: 100
save_strategy: "steps"
save_steps: 100
save_total_limit: 5
