import abc
import os

import anthropic
import openai
import together
from langchain.prompts import PromptTemplate
from tenacity import retry, stop_after_attempt, wait_fixed

from data import ContentTextMessage, Conversation

STOP = ["\n"]
top_p = 0.7
top_k = 40
MAX_TOKENS = 512


class LLMChat(abc.ABC):
    def __init__(self, max_retries: int, wait_seconds: int, temperature: float, seed: int):
        """
        Initialize the LLM chat object.

        Args:
            max_retries (int): Max number of API calls allowed before giving up.
            wait_seconds (int): Number of seconds to wait between API calls.
            temperature (float): Temperature parameter for sampling.
            seed (int): Seed for random number generator, passed to the model if applicable.
        """
        self.max_retries = max_retries
        self.wait_seconds = wait_seconds
        self.temperature = temperature
        self.seed = seed

    @abc.abstractmethod
    def generate_response(self, conv: Conversation) -> str:
        """
        Generate a response for the conversation.

        Args:
            conv (Conversation): The conversation object.

        Returns:
            str: The response generated by the model.
        """
        pass


class CommonLLMChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)

        self.model_name = model_name
        self.stream_generations = stream_generations
        self.client = None
    
    @abc.abstractmethod
    def _call_api(self, conv: Conversation) -> str:
        pass

    def generate_response(self, conv: Conversation) -> str:
        assert self.client is not None, "Client is not initialized."

        # max_retries and wait_seconds are object attributes, and cannot be written around the generate_response function
        # So we need to wrap the _call_api function with the retry decorator
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api_wrapper(conv: Conversation) -> str:
            return self._call_api(conv)

        return _call_api_wrapper(conv)


class OpenAIChat(CommonLLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, stream_generations)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def _call_api(self, conv: Conversation) -> str:
        # https://platform.openai.com/docs/api-reference/introduction
        # Convert conv to OpenAI format
        formatted_messages = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        formatted_messages.append({"role": message.role, "content": [{"type": "text", "text": text}]})

        completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=formatted_messages,
            temperature=self.temperature,
            seed=self.seed,
            stream=self.stream_generations,
        )
        return completion if self.stream_generations else completion.choices[0].message.content


class TogetherChat(OpenAIChat):
    # Together follows the same API as OpenAI, just initialize the client
    # https://github.com/togethercomputer/together-python
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        assert not model_name.startswith("together:"), "model_name must not start with 'together:', remove it then initialize the TogetherChat object."
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, stream_generations)
        self.client = together.Together(api_key=os.getenv("TOGETHER_API_KEY"))


class GeminiChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        import google.generativeai as genai

        self.model_name = model_name
        self.stream_generations = stream_generations
        self.google_model = genai.GenerativeModel(self.model_name)

    def generate_response(self, conv: Conversation) -> str:
        # TODO
        """
        Ref:
        - https://ai.google.dev/gemini-api/docs/models/gemini
        """
        gemini_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        gemini_conv.append({"role": message.role, "parts": [text]})

        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            import pdb; pdb.set_trace()

            completion = self.google_model.generate_content(
                contents=gemini_conv,
                generation_config={
                    'temperature': self.temperature,
                    'top_p': top_p,
                    # 'top_k': top_k, # NOTE: API does not suport topK>40
                    'max_output_tokens': MAX_TOKENS,
                    'stop_sequences': STOP,
                    # NOTE: API does not support seed, repetition_penalty
                },
                stream=self.stream_generations
            )
            return completion if self.stream_generations else completion.text

        return _call_api(conv)
    

class AnthropicChat(CommonLLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, stream_generations)
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    def _call_api(self, conv: Conversation) -> str:
        # https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages
        # Convert conv to Anthropic format
        formatted_messages = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        formatted_messages.append({"role": message.role, "content": [{"type": "text", "text": text}]})

        response = self.client.messages.create(
            model=self.model_name,
            messages=formatted_messages,
            temperature=self.temperature,
            top_p=top_p,
            top_k=top_k,
            max_tokens=MAX_TOKENS,
            stream=self.stream_generations,
        )
        return response.content[0].text


# class AnthropicChat(LLMChat):
#     def __init__(
#         self,
#         model_name: str,
#         max_retries: int,
#         wait_seconds: int,
#         temperature: float,
#         seed: int,
#         stream_generations: bool,
#     ):
#         """
#         Args:
#             model_name (str): The model name to use.
#             stream_generations (bool): Flag to enable streaming generations.
#         """
#         super().__init__(max_retries, wait_seconds, temperature, seed)
#         from anthropic import Anthropic

#         self.model_name = model_name
#         self.stream_generations = stream_generations
#         self.anthropic_model = Anthropic()

#     def generate_response(self, conv: Conversation) -> str:
#         # TODO
#         """
#         Ref: 
#         - https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages 
#         """
#         claude_conv = []
#         for message in conv.messages:
#             for content in message.content:
#                 match content:
#                     case ContentTextMessage(text=text):
#                         claude_conv.append({"role": message.role, "content": [{"type": "text", "text": text}]})


#         # Wrap retry params inside generate_response
#         @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
#         def _call_api(conv: Conversation) -> str:
#             import pdb; pdb.set_trace()
#             completion = self.anthropic_model.messages.create(
#                 model=self.model_name,
#                 messages=claude_conv,
#                 temperature=self.temperature,
#                 top_p=top_p,
#                 top_k=top_k,
#                 # NOTE: repetition_penalty is not supported by Anthropic's API
#                 # NOTE: by default, the model will stop at the end of the turn
#                 max_tokens=MAX_TOKENS,
#                 # NOTE: seed is not supported by Anthropic's API
#                 stream = self.stream_generations
#             )
#             return completion if self.stream_generations else completion.content[0].text

#         return _call_api(conv)


REACT_INSTRUCTION = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be two types: 
(1) RetrieveArticle[entity], which searches the exact entity on Wikipedia and returns the page if it exists. If not, it will return that the page does not exist.
(2) Finish[answer], which returns the answer and finishes the task.
You may take as many steps as necessary.
Here are some examples:
{examples}
(END OF EXAMPLES)
Question: {question}{scratchpad}"""


class LLMPrompts(abc.ABC):
    @abc.abstractmethod
    def react_agent_prompt(self) -> PromptTemplate:
        pass


class OpenAIPrompts(LLMPrompts):
    def __init__(self) -> None:
        pass
    
    def react_agent_prompt(self):
        return PromptTemplate(
            input_variables=["examples", "question", "scratchpad"],
            template=REACT_INSTRUCTION,
        )


def get_llm(model_name: str, model_kwargs: dict) -> tuple[LLMChat, LLMPrompts]:
    match model_name:
        case "gpt-3.5-turbo" | "gpt-4o-mini":
            return OpenAIChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        case "llama-3.1-8b":
            raise NotImplementedError("LLama-3.1-8b model is not yet supported.")
        case "claude-3-opus-20240229":
            return AnthropicChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        case "gemini-1.5-flash":
            return GeminiChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        # case :
        #     raise ValueError(f"Unknown model name: {model_name}")