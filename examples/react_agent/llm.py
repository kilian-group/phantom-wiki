import abc
import os

from langchain.prompts import PromptTemplate
from tenacity import retry, stop_after_attempt, wait_fixed

from data import ContentTextMessage, Conversation

STOP = ["\n"]
top_p = 0.7
top_k = 40
MAX_TOKENS = 512


class LLMChat(abc.ABC):
    def __init__(self, max_retries: int, wait_seconds: int, temperature: float, seed: int):
        """
        Initialize the LLM chat object.

        Args:
            max_retries (int): Max number of API calls allowed before giving up.
            wait_seconds (int): Number of seconds to wait between API calls.
            temperature (float): Temperature parameter for sampling.
            seed (int): Seed for random number generator, passed to the model if applicable.
        """
        self.max_retries = max_retries
        self.wait_seconds = wait_seconds
        self.temperature = temperature
        self.seed = seed

    @abc.abstractmethod
    def generate_response(self, conv: Conversation) -> str:
        """
        Generate a response for the conversation.

        Args:
            conv (Conversation): The conversation object.

        Returns:
            str: The response generated by the model.
        """
        pass


class OpenAIChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        import openai

        self.model_name = model_name
        self.stream_generations = stream_generations
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_response(self, conv: Conversation) -> str:
        # Convert conv to OpenAI format
        openai_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        openai_conv.append({"role": message.role, "content": [{"type": "text", "text": text}]})

        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            completion = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=openai_conv,
                temperature=self.temperature,
                seed=self.seed,
                stream=self.stream_generations,
            )
            return completion if self.stream_generations else completion.choices[0].message.content

        return _call_api(conv)
    

class GeminiChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        import google.generativeai as genai

        self.model_name = model_name
        self.stream_generations = stream_generations
        self.google_model = genai.GenerativeModel(self.model_name)

    def generate_response(self, conv: Conversation) -> str:
        # TODO
        """
        Ref:
        - https://ai.google.dev/gemini-api/docs/models/gemini
        """
        gemini_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        gemini_conv.append({"role": message.role, "parts": [text]})

        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            import pdb; pdb.set_trace()

            completion = self.google_model.generate_content(
                contents=gemini_conv,
                generation_config={
                    'temperature': self.temperature,
                    'top_p': top_p,
                    # 'top_k': top_k, # NOTE: API does not suport topK>40
                    'max_output_tokens': MAX_TOKENS,
                    'stop_sequences': STOP,
                    # NOTE: API does not support seed, repetition_penalty
                },
                stream=self.stream_generations
            )
            return completion if self.stream_generations else completion.text

        return _call_api(conv)
    
class AnthropicChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        from anthropic import Anthropic

        self.model_name = model_name
        self.stream_generations = stream_generations
        self.anthropic_model = Anthropic()

    def generate_response(self, conv: Conversation) -> str:
        # TODO
        """
        Ref: 
        - https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages 
        """
        claude_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        claude_conv.append({"role": message.role, "content": [{"type": "text", "text": text}]})


        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            import pdb; pdb.set_trace()
            completion = self.anthropic_model.messages.create(
                model=self.model_name,
                messages=claude_conv,
                temperature=self.temperature,
                top_p=top_p,
                top_k=top_k,
                # NOTE: repetition_penalty is not supported by Anthropic's API
                # NOTE: by default, the model will stop at the end of the turn
                max_tokens=MAX_TOKENS,
                # NOTE: seed is not supported by Anthropic's API
                stream = self.stream_generations
            )
            return completion if self.stream_generations else completion.content[0].text

        return _call_api(conv)


REACT_INSTRUCTION = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be two types: 
(1) RetrieveArticle[entity], which searches the exact entity on Wikipedia and returns the page if it exists. If not, it will return that the page does not exist.
(2) Finish[answer], which returns the answer and finishes the task.
You may take as many steps as necessary.
Here are some examples:
{examples}
(END OF EXAMPLES)
Question: {question}{scratchpad}"""


class LLMPrompts(abc.ABC):
    @abc.abstractmethod
    def react_agent_prompt(self) -> PromptTemplate:
        pass


class OpenAIPrompts(LLMPrompts):
    def __init__(self) -> None:
        pass
    
    def react_agent_prompt(self):
        return PromptTemplate(
            input_variables=["examples", "question", "scratchpad"],
            template=REACT_INSTRUCTION,
        )


def get_llm(model_name: str, model_kwargs: dict) -> tuple[LLMChat, LLMPrompts]:
    match model_name:
        case "gpt-3.5-turbo" | "gpt-4o-mini":
            return OpenAIChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        case "llama-3.1-8b":
            raise NotImplementedError("LLama-3.1-8b model is not yet supported.")
        case "claude-3-opus-20240229":
            return AnthropicChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        case "gemini-1.5-flash":
            return GeminiChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        # case :
        #     raise ValueError(f"Unknown model name: {model_name}")