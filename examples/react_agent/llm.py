import abc
import os

import openai
from langchain.prompts import PromptTemplate
from tenacity import retry, stop_after_attempt, wait_fixed

from data import ContentTextMessage, Conversation


class LLMChat(abc.ABC):
    def __init__(self, max_retries: int, wait_seconds: int, temperature: float, seed: int):
        """
        Initialize the LLM chat object.

        Args:
            max_retries (int): Max number of API calls allowed before giving up.
            wait_seconds (int): Number of seconds to wait between API calls.
            temperature (float): Temperature parameter for sampling.
            seed (int): Seed for random number generator, passed to the model if applicable.
        """
        self.max_retries = max_retries
        self.wait_seconds = wait_seconds
        self.temperature = temperature
        self.seed = seed

    @abc.abstractmethod
    def generate_response(self, conv: Conversation) -> str:
        """
        Generate a response for the conversation.

        Args:
            conv (Conversation): The conversation object.

        Returns:
            str: The response generated by the model.
        """
        pass


class OpenAIChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        self.model_name = model_name
        self.stream_generations = stream_generations
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_response(self, conv: Conversation) -> str:
        # Convert conv to OpenAI format
        openai_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        openai_conv.append({"role": message.role, "content": [{"type": "text", "text": text}]})

        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            completion = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=openai_conv,
                temperature=self.temperature,
                seed=self.seed,
                stream=self.stream_generations,
            )
            return completion if self.stream_generations else completion.choices[0].message.content

        return _call_api(conv)


REACT_INSTRUCTION = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be two types: 
(1) RetrieveArticle[entity], which searches the exact entity on Wikipedia and returns the page if it exists. If not, it will return that the page does not exist.
(2) Finish[answer], which returns the answer and finishes the task.
You may take as many steps as necessary.
Here are some examples:
{examples}
(END OF EXAMPLES)
Question: {question}{scratchpad}"""


class LLMPrompts(abc.ABC):
    @abc.abstractmethod
    def react_agent_prompt(self) -> PromptTemplate:
        pass


class OpenAIPrompts(LLMPrompts):
    def __init__(self) -> None:
        pass
    
    def react_agent_prompt(self):
        return PromptTemplate(
            input_variables=["examples", "question", "scratchpad"],
            template=REACT_INSTRUCTION,
        )


def get_llm(model_name: str, model_kwargs: dict) -> tuple[LLMChat, LLMPrompts]:
    match model_name:
        case "gpt-3.5-turbo" | "gpt-4o-mini":
            return OpenAIChat(model_name=model_name, **model_kwargs), OpenAIPrompts()
        case "llama-3.1-8b":
            raise NotImplementedError("LLama-3.1-8b model is not yet supported.")
        case _:
            raise ValueError(f"Unknown model name: {model_name}")