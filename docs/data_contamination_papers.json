[
  {
    "title": "Language Models are Few-Shot Learners",
    "tags": "Reactive|Analysis",
    "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
    "blog": "",
    "leaderboard": "",
    "paper": "https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": "This paper is not all about data contamination. Still, it is the very first paper that officially discusses the data contamination problem and presents an N-gram approach to identify the contamination risk of benchmarks (Appendix C)."
  },
  {
    "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
    "tags": "Reactive|Analysis",
    "authors": "Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2021.emnlp-main.98/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": "This paper only discusses benchmark contamination of pretraining corpora in Section 4.2. The authors also adopt a more aggressive N-gram matching method for contamination detection."
  },
  {
    "title": "Data Contamination: From Memorization to Exploitation",
    "tags": "Analysis",
    "authors": "Inbal Magar, Roy Schwartz",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2022.acl-short.18/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Holistic Evaluation of Language Models",
    "tags": "Analysis|Tool",
    "authors": "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2211.09110",
    "code": "https://github.com/stanford-crfm/helm",
    "website": "https://crfm.stanford.edu/helm/classic/latest/",
    "data": "",
    "abstract": "",
    "notes": "This paper is not all about data contamination. It documents known evidence of contamination when possible (Appendix G)."
  },
  {
    "title": "Can we trust the evaluation on ChatGPT?",
    "tags": "Analysis",
    "authors": "Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2023.trustnlp-1.5/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Koala: An Index for Quantifying Overlaps with Pre-training Corpora",
    "tags": "Tool",
    "authors": "Thuy-Trang Vu, Xuanli He, Gholamreza Haffari, Ehsan Shareghi",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2023.emnlp-demo.7/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    "tags": "Preventative",
    "authors": "Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2023.emnlp-main.308/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
    "tags": "Preventative|Dataset",
    "authors": "Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2306.04181",
    "code": "",
    "website": "",
    "data": "https://lmexam.com/",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "CLEVA: Chinese Language Models EVAluation Platform",
    "tags": "Preventative",
    "authors": "Yanyang Li, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael Lyu, Liwei Wang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2023.emnlp-demo.17/",
    "code": "",
    "website": "http://www.lavicleva.com/",
    "data": "",
    "abstract": "",
    "notes": "This paper is not all about data contamination. It presents methods for alleviating the contamination issue from both the benchmark construction and leaderboard maintenance perspectives."
  },
  {
    "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
    "tags": "Reactive|Tool",
    "authors": "Shahriar Golchin, Mihai Surdeanu",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2308.08493",
    "code": "https://github.com/shahriargolchin/time-travel-in-llms",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
    "tags": "Reactive",
    "authors": "Yucheng Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2309.10677",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "DyVal: Graph-informed Dynamic Evaluation of Large Language Models",
    "tags": "Preventative|Tool",
    "authors": "Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2309.17167",
    "code": "https://github.com/microsoft/promptbench",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination",
    "tags": "Analysis",
    "authors": "Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley",
    "blog": "",
    "leaderboard": "",
    "paper": "https://openreview.net/forum?id=m2NVG4Htxs",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",
    "tags": "Preventative|Tool",
    "authors": "Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, Kang Liu",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2310.15147",
    "code": "https://github.com/lfy79001/S3Eval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Detecting Pretraining Data from Large Language Models",
    "tags": "Reactive|Tool|Dataset",
    "authors": "Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2310.16789",
    "code": "https://github.com/swj0419/detect-pretrain-code",
    "website": "https://swj0419.github.io/detect-pretrain.github.io/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Proving Test Set Contamination in Black Box Language Models",
    "tags": "Reactive",
    "authors": "Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2310.17623",
    "code": "https://github.com/tatsu-lab/test_set_contamination",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "An Open Source Data Contamination Report for Large Language Models",
    "tags": "Analysis|Tool",
    "authors": "Yucheng Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2310.17589",
    "code": "https://github.com/liyucheng09/Contamination_Detector",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Skywork: A More Open Bilingual Foundation Model",
    "tags": "Reactive|Analysis|Dataset",
    "authors": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei L\u00fc, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2310.19341",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": "This paper is not all about data contamination. It releases a new dataset augmented from GSM8K to detect the contamination risk of other models in GSM8K (Section 5)."
  },
  {
    "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
    "tags": "Analysis|Tool",
    "authors": "Oscar Sainz, Jon Ander Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2023.findings-emnlp.722/",
    "code": "https://github.com/hitz-zentroa/lm-contamination",
    "website": "https://hitz-zentroa.github.io/lm-contamination/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
    "tags": "Analysis",
    "authors": "Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2311.01964",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
    "tags": "Analysis|Tool",
    "authors": "Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2311.04850",
    "code": "https://github.com/lm-sys/llm-decontaminator",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
    "tags": "Reactive",
    "authors": "Shahriar Golchin, Mihai Surdeanu",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2311.06233",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
    "tags": "Reactive|Analysis",
    "authors": "Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2311.09783",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
    "tags": "Preventative|Tool|Dataset",
    "authors": "Yucheng Li, Frank Guerin, Chenghua Lin",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2312.12343",
    "code": "https://github.com/liyucheng09/LatestEval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
    "tags": "Preventative|Tool|Dataset",
    "authors": "Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2312.14890",
    "code": "https://github.com/casmlab/NPHardEval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
    "tags": "Reactive|Analysis",
    "authors": "Changmao Li, Jeffrey Flanigan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2312.16337",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Investigating Data Contamination for Pre-training Language Models",
    "tags": "Analysis",
    "authors": "Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2401.06059",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
    "tags": "Dataset|Reactive",
    "authors": "Andr\u00e9 V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.09910",
    "code": "https://github.com/LeiLiLab/DE-COP",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
    "tags": "Analysis",
    "authors": "Simone Balloccu, Patr\u00edcia Schmidtov\u00e1, Mateusz Lango, Ond\u0159ej Du\u0161ek",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.03927",
    "code": "",
    "website": "https://leak-llm.github.io/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation",
    "tags": "Analysis|Dataset|Reactive",
    "authors": "Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.08100",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
    "tags": "Dataset|Preventative",
    "authors": "Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, Xuanjing Huang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.11443",
    "code": "https://github.com/NanshineLoong/Self-Evolving-Benchmark",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation",
    "tags": "Preventative",
    "authors": "Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.11894",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
    "tags": "Preventative|Tool",
    "authors": "Xiang Li, Yunshi Lan, Chao Yang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.13125",
    "code": "https://github.com/Ashura5/TreeEval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents",
    "tags": "Preventative",
    "authors": "Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.14865",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
    "tags": "Preventative",
    "authors": "Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.15043",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
    "tags": "Reactive|Dataset",
    "authors": "Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2402.15938",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
    "tags": "Preventative",
    "authors": "Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma, Kashish Mittal, Manohar Swaminathan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.00393",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models",
    "tags": "Preventative",
    "authors": "Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.01777",
    "code": "https://github.com/lizhouf/NPHardEval4V",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
    "tags": "Analysis",
    "authors": "Martin Riddell, Ansong Ni, Arman Cohan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.04811",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
    "tags": "Analysis|Tool",
    "authors": "Sebastian Bordt, Harsha Nori, Rich Caruana",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.06644",
    "code": "https://github.com/interpretml/LLM-Tabular-Memorization-Checker",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
    "tags": "Preventative|Dataset",
    "authors": "Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.07974",
    "code": "",
    "website": "https://livecodebench.github.io/",
    "data": "https://huggingface.co/livecodebench",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
    "tags": "Preventative|Dataset",
    "authors": "Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Kaifeng Yun, Linlu GONG, Nianyi Lin, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Xu Bin, Jie Tang, Juanzi Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://openreview.net/forum?id=AqN23oqraW",
    "code": "",
    "website": "https://kola.xlore.cn/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
    "tags": "Preventative|Tool",
    "authors": "Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2403.19114",
    "code": "https://github.com/evo-eval/evoeval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
    "tags": "Preventative|Dataset",
    "authors": "Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, Zhi Jin",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.00599",
    "code": "https://github.com/seketeam/EvoCodeBench",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library",
    "tags": "Survey|Tool",
    "authors": "Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.00699",
    "code": "https://github.com/ntunlp/LLMSanitize",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
    "tags": "Reactive|Tool",
    "authors": "Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.02936",
    "code": "https://github.com/zjysteven/mink-plus-plus",
    "website": "https://zjysteven.github.io/mink-plus-plus/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
    "tags": "Tool",
    "authors": "Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.06003",
    "code": "https://github.com/WisdomShell/FreeEval",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Benchmarking Benchmark Leakage in Large Language Models",
    "tags": "Analysis|Tool",
    "authors": "Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.18824",
    "code": "https://github.com/GAIR-NLP/benbench",
    "website": "https://gair-nlp.github.io/benbench/",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
    "tags": "Analysis",
    "authors": "Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2405.00332",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
    "tags": "Reactive|Analysis|Tool",
    "authors": "Shangqing Tu, Kejian Zhu, Yushi Bai, Zijun Yao, Lei Hou, Juanzi Li",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2406.04197",
    "code": "https://github.com/THU-KEG/DICE",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
    "tags": "Preventative|Dataset",
    "authors": "Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum",
    "blog": "",
    "leaderboard": "",
    "paper": "https://livebench.ai/livebench.pdf",
    "code": "https://github.com/LiveBench/LiveBench",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Evaluating Chinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment",
    "tags": "Analysis",
    "authors": "Chuang Liu, Renren Jin, Mark Steedman, Deyi Xiong",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.conda-1.1/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Scaling Laws for Data Poisoning in LLMs",
    "tags": "Analysis",
    "authors": "Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2408.02946",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "LLM Dataset Inference: Did you train on my dataset?",
    "tags": "Reactive",
    "authors": "Pratyush Maini, Hengrui Jia, Nicolas Papernot, Adam Dziedzic",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2406.06443",
    "code": "https://github.com/pratyushmaini/llm_dataset_inference/",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Rethinking LLM Memorization through the Lens of Adversarial Compression",
    "tags": "Reactive|Analysis",
    "authors": "Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2404.15146",
    "code": "",
    "website": "https://locuslab.github.io/acr-memorization",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
    "tags": "Dataset",
    "authors": "Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2401.06121",
    "code": "",
    "website": "https://locuslab.github.io/tofu/",
    "data": "",
    "abstract": "",
    "notes": "Unlearning could be an important technique to mitigate contamination after it happens."
  },
  {
    "title": "Train-to-Test Contamination in Code Generation Evaluations",
    "tags": "Dataset",
    "authors": "Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gall\u00e9",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2407.07565",
    "code": "",
    "website": "",
    "data": "https://huggingface.co/datasets/CohereForAI/lbpp",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts",
    "tags": "Dataset|Reactive|Preventative",
    "authors": "Jacob Haimes, Cenny Wenner, Kunvar Thaman, Vassil Tashev, Clement Neo, Esben Kran, Jason Hoelscher-Obermaier",
    "blog": "",
    "leaderboard": "",
    "paper": "https://jacob-haimes.github.io/uploads/benchmark-inflation_extended-abstract_dmlr_v1.0.pdf",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Confounders in Instance Variation for the Analysis of Data Contamination",
    "tags": "Reactive|Analysis",
    "authors": "Behzad Mehrbakhsh, Dario Garigliotti, Fernando Mart\u00ednez-Plumed, Jose Hernandez-Orallo",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.conda-1.2/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
    "tags": "Survey",
    "authors": "Chunyuan Deng, Yilun Zhao, Yuzhao Heng, Yitong Li, Jiannan Cao, Xiangru Tang, Arman Cohan",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.findings-acl.951/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "A Taxonomy for Data Contamination in Large Language Models",
    "tags": "Analysis",
    "authors": "Medha Palavalli, Amanda Bertsch, Matthew Gormley",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.conda-1.3/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Proving membership in LLM pretraining data via data watermarks",
    "tags": "Preventative",
    "authors": "Johnny Wei, Ryan Yixiang Wang, Robin Jia",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.findings-acl.788/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Data Contamination Calibration for Black-box LLMs",
    "tags": "Reactive",
    "authors": "Wentao Ye, Jiaqi Hu, Liyao Li, Haobo Wang, Gang Chen, Junbo Zhao",
    "blog": "",
    "leaderboard": "",
    "paper": "https://aclanthology.org/2024.findings-acl.644/",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
    "tags": "Preventative",
    "authors": "Zhehao Zhang, Jiaao Chen, Diyi Yang",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2406.17271",
    "code": "https://github.com/SALT-NLP/DARG",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
    "tags": "Preventative",
    "authors": "Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu",
    "blog": "",
    "leaderboard": "",
    "paper": "https://arxiv.org/abs/2406.17681",
    "code": "https://github.com/qbetterk/VarBench",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  },
  {
    "title": "Decentralized Arena via Collective LLM Intelligence",
    "tags": "Preventative",
    "authors": "Yanbin Yin, Zhen Wang, Kun Zhou, Xiangdong Zhang, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Xing Eric P., Zhengzhong Liu, Haojian Jin, Zhiting Hu",
    "blog": "https://de-arena.maitrix.org/",
    "leaderboard": "https://huggingface.co/spaces/LLM360/de-arena",
    "paper": "",
    "code": "",
    "website": "",
    "data": "",
    "abstract": "",
    "notes": ""
  }
]